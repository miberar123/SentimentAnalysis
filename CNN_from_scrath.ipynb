{"cells":[{"cell_type":"code","execution_count":1,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":96},"executionInfo":{"elapsed":3916,"status":"ok","timestamp":1577205441348,"user":{"displayName":"Ana Laguna Pradas","photoUrl":"https://lh3.googleusercontent.com/a-/AAuE7mBSGptTK-p_QVjyTUyfgnUmNBGxmgf1siaiZ-4m=s64","userId":"04586459829917851529"},"user_tz":-60},"id":"p5osP61qxBKu","outputId":"d8a5f6db-4cff-4bf3-8b6b-67f4b0922fcb"},"outputs":[],"source":["import tensorflow.keras\n","import os\n","import cv2\n","import numpy as np\n","\n","#keras.__version__"]},{"cell_type":"markdown","metadata":{"collapsed":true,"id":"8exN-nnKxBLA"},"source":["## CNN from scratch\n","Un modelo from scratch es una red neuronal, la cual le definimos nosotros las capas que queremos que tenga. El dataset que tengamos influirá en gran medida las capas que tendremos que definir\n","\n"]},{"cell_type":"markdown","metadata":{},"source":["Este modelo es una red neuronal convolucional (CNN) diseñada para tareas de clasificación de imágenes en 8 clases diferentes. Veamos paso a paso:\n","\n","Capa de entrada: Esta red espera imágenes en escala de grises con dimensiones de 224x224 píxeles, ya que la forma de entrada especificada es (224, 224, 1). El tamaño del lote no se especifica aquí.\n","\n","- **Capas convolucionales**: El modelo comienza con una capa convolucional que tiene 32 filtros de tamaño 3x3 y utiliza la función de activación ReLU. Luego, se aplica una capa de MaxPooling con una ventana de 2x2 para reducir la dimensionalidad. Se repite este proceso dos veces más, aumentando el número de filtros en cada capa convolucional (64 y luego 128), lo que permite que la red aprenda características más complejas a medida que profundiza.\n","\n","- **Capas densas**: Después de las capas convolucionales y de reducción de dimensionalidad, los datos se aplastan (flatten) para ser pasados a través de capas densas. La primera capa densa tiene 64 unidades de salida con activación ReLU.\n","\n","- **Capa de salida**: Finalmente, hay una capa densa de salida con 8 unidades (una para cada clase de salida) y utiliza la función de activación softmax para asignar probabilidades a cada clase.\n","\n","Este modelo cuando lo entrenamos daba sobre entrenamiento por lo qeu decisimos desarrollar otro con maenos capas par aevitar dicho problema"]},{"cell_type":"code","execution_count":2,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":234},"executionInfo":{"elapsed":3070,"status":"ok","timestamp":1581200819174,"user":{"displayName":"Ana Laguna Pradas","photoUrl":"https://lh3.googleusercontent.com/a-/AAuE7mBSGptTK-p_QVjyTUyfgnUmNBGxmgf1siaiZ-4m=s64","userId":"04586459829917851529"},"user_tz":-60},"id":"IiHUk6IBxBLB","outputId":"5103e3a4-19e5-4fd4-9c10-4d47eb667166"},"outputs":[{"name":"stdout","output_type":"stream","text":["Model: \"sequential\"\n","_________________________________________________________________\n"," Layer (type)                Output Shape              Param #   \n","=================================================================\n"," conv2d (Conv2D)             (None, 222, 222, 32)      320       \n","                                                                 \n"," max_pooling2d (MaxPooling2D  (None, 111, 111, 32)     0         \n"," )                                                               \n","                                                                 \n"," conv2d_1 (Conv2D)           (None, 109, 109, 64)      18496     \n","                                                                 \n"," max_pooling2d_1 (MaxPooling  (None, 54, 54, 64)       0         \n"," 2D)                                                             \n","                                                                 \n"," conv2d_2 (Conv2D)           (None, 52, 52, 128)       73856     \n","                                                                 \n"," max_pooling2d_2 (MaxPooling  (None, 26, 26, 128)      0         \n"," 2D)                                                             \n","                                                                 \n"," flatten (Flatten)           (None, 86528)             0         \n","                                                                 \n"," dense (Dense)               (None, 64)                5537856   \n","                                                                 \n"," dense_1 (Dense)             (None, 8)                 520       \n","                                                                 \n","=================================================================\n","Total params: 5,631,048\n","Trainable params: 5,631,048\n","Non-trainable params: 0\n","_________________________________________________________________\n"]}],"source":["from keras import layers\n","from keras import models\n","\n","\n","model = models.Sequential()\n","\n","# Primera capa convolucional con 32 filtros de tamaño 3x3 y activación relu\n","model.add(layers.Conv2D(32, (3, 3), activation='relu', input_shape=(224, 224, 1)))\n","\n","# Capa de max pooling con ventana 2x2\n","model.add(layers.MaxPooling2D((2, 2)))\n","\n","# Segunda capa convolucional con 64 filtros de tamaño 3x3 y activación relu\n","model.add(layers.Conv2D(64, (3, 3), activation='relu'))\n","\n","# Capa de max pooling con ventana 2x2\n","model.add(layers.MaxPooling2D((2, 2)))\n","\n","# Tercera capa convolucional con 128 filtros de tamaño 3x3 y activación relu\n","model.add(layers.Conv2D(128, (3, 3), activation='relu'))\n","\n","# Capa de max pooling con ventana 2x2\n","model.add(layers.MaxPooling2D((2, 2)))\n","\n","# Aplanar los datos para la capa densa\n","model.add(layers.Flatten())\n","\n","# Capa densa con 64 unidades de salida y activación relu\n","model.add(layers.Dense(64, activation='relu'))\n","\n","# Capa de salida con 8 unidades de salida (para clasificación en 8 clases) y activación softmax\n","model.add(layers.Dense(8, activation='softmax'))\n","\n","# Display model summary\n","model.summary()"]},{"cell_type":"markdown","metadata":{},"source":["Este modelo ya dio unoos resultados que no tenían sobre entrenamint debido a la disminución de capas.\n","\n","- **Capa de entrada**: Esta red espera imágenes en escala de grises con dimensiones de 224x224 píxeles, ya que la forma de entrada especificada es (224, 224, 1). El tamaño del lote no se especifica aquí.\n","\n","- **Capas convolucionales**: El modelo comienza con una capa convolucional que tiene 32 filtros de tamaño 3x3 y utiliza la función de activación ReLU. Luego, se aplica una capa de MaxPooling con una ventana de 2x2 para reducir la dimensionalidad. Se repite este proceso dos veces más, aumentando el número de filtros en cada capa convolucional (64 y luego 128), lo que permite que la red aprenda características más complejas a medida que profundiza.\n","\n","- **Capas densas**: Después de las capas convolucionales y de reducción de dimensionalidad, los datos se aplastan (flatten) para ser pasados a través de capas densas. La primera capa densa tiene 64 unidades de salida con activación ReLU.\n","\n","- **Capa de salida**: Finalmente, hay una capa densa de salida con 8 unidades (una para cada clase de salida) y utiliza la función de activación softmax para asignar probabilidades a cada clase."]},{"cell_type":"code","execution_count":1,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["Model: \"sequential\"\n","_________________________________________________________________\n"," Layer (type)                Output Shape              Param #   \n","=================================================================\n"," conv2d (Conv2D)             (None, 222, 222, 32)      320       \n","                                                                 \n"," max_pooling2d (MaxPooling2D  (None, 111, 111, 32)     0         \n"," )                                                               \n","                                                                 \n"," conv2d_1 (Conv2D)           (None, 109, 109, 64)      18496     \n","                                                                 \n"," max_pooling2d_1 (MaxPooling  (None, 54, 54, 64)       0         \n"," 2D)                                                             \n","                                                                 \n"," conv2d_2 (Conv2D)           (None, 52, 52, 128)       73856     \n","                                                                 \n"," max_pooling2d_2 (MaxPooling  (None, 26, 26, 128)      0         \n"," 2D)                                                             \n","                                                                 \n"," flatten (Flatten)           (None, 86528)             0         \n","                                                                 \n"," dense (Dense)               (None, 64)                5537856   \n","                                                                 \n"," dropout (Dropout)           (None, 64)                0         \n","                                                                 \n"," dense_1 (Dense)             (None, 8)                 520       \n","                                                                 \n","=================================================================\n","Total params: 5,631,048\n","Trainable params: 5,631,048\n","Non-trainable params: 0\n","_________________________________________________________________\n"]}],"source":["from keras import layers\n","from keras import models\n","\n","model_1 = models.Sequential()\n","\n","model_1.add(layers.Conv2D(32, (3, 3), activation='relu', input_shape=(224, 224, 1)))\n","model_1.add(layers.MaxPooling2D((2, 2)))\n","\n","model_1.add(layers.Conv2D(64, (3, 3), activation='relu'))\n","model_1.add(layers.MaxPooling2D((2, 2)))\n","\n","model_1.add(layers.Conv2D(128, (3, 3), activation='relu'))\n","model_1.add(layers.MaxPooling2D((2, 2)))\n","\n","model_1.add(layers.Flatten())\n","model_1.add(layers.Dense(64, activation='relu'))\n","model_1.add(layers.Dropout(0.5))  # Agregar Dropout para reducir el sobreajuste\n","model_1.add(layers.Dense(8, activation='softmax'))\n","model_1.summary()"]},{"cell_type":"markdown","metadata":{"id":"yKGpTXtbxBLO"},"source":["Aquí ponemos si queremos usar el modelo 1 o si queremos el modelo normal"]},{"cell_type":"code","execution_count":4,"metadata":{},"outputs":[],"source":["model = model_1"]},{"cell_type":"markdown","metadata":{},"source":["Usando la función summary podemos observar de manera resumida las cpas de nuestro modelo "]},{"cell_type":"code","execution_count":5,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":319},"executionInfo":{"elapsed":567,"status":"ok","timestamp":1581200826037,"user":{"displayName":"Ana Laguna Pradas","photoUrl":"https://lh3.googleusercontent.com/a-/AAuE7mBSGptTK-p_QVjyTUyfgnUmNBGxmgf1siaiZ-4m=s64","userId":"04586459829917851529"},"user_tz":-60},"id":"GhjmX5HIxBLP","outputId":"9fc2bf3f-f6bf-4701-ff7d-bca57f6d3ea0"},"outputs":[{"name":"stdout","output_type":"stream","text":["Model: \"sequential_1\"\n","_________________________________________________________________\n"," Layer (type)                Output Shape              Param #   \n","=================================================================\n"," conv2d_3 (Conv2D)           (None, 222, 222, 32)      320       \n","                                                                 \n"," max_pooling2d_3 (MaxPooling  (None, 111, 111, 32)     0         \n"," 2D)                                                             \n","                                                                 \n"," conv2d_4 (Conv2D)           (None, 109, 109, 64)      18496     \n","                                                                 \n"," max_pooling2d_4 (MaxPooling  (None, 54, 54, 64)       0         \n"," 2D)                                                             \n","                                                                 \n"," conv2d_5 (Conv2D)           (None, 52, 52, 128)       73856     \n","                                                                 \n"," max_pooling2d_5 (MaxPooling  (None, 26, 26, 128)      0         \n"," 2D)                                                             \n","                                                                 \n"," flatten_1 (Flatten)         (None, 86528)             0         \n","                                                                 \n"," dense_2 (Dense)             (None, 64)                5537856   \n","                                                                 \n"," dropout (Dropout)           (None, 64)                0         \n","                                                                 \n"," dense_3 (Dense)             (None, 8)                 520       \n","                                                                 \n","=================================================================\n","Total params: 5,631,048\n","Trainable params: 5,631,048\n","Non-trainable params: 0\n","_________________________________________________________________\n"]}],"source":["model.summary()"]},{"cell_type":"markdown","metadata":{},"source":["## Parseo para cargar los datos.\n","\n","Para cargar los datos hemos definido esat funcion para cargarlos de forma que podamos optener las imagenes y las etiquetas."]},{"cell_type":"code","execution_count":7,"metadata":{},"outputs":[],"source":["def load_images_from_subfolders(folder):\n","    images = []\n","    labels = []\n","    categories = sorted(os.listdir(folder))\n","    for category_idx, category in enumerate(categories):\n","        category_path = os.path.join(folder, category)\n","        for filename in os.listdir(category_path):\n","            img = cv2.imread(os.path.join(category_path, filename), cv2.IMREAD_GRAYSCALE)\n","            if img is not None:\n","                images.append(img)\n","                labels.append(category_idx)  # Use index of the category as label\n","    return np.array(images), np.array(labels)\n"]},{"cell_type":"markdown","metadata":{"id":"xVOlGImbxBLr"},"source":["Aquí cargamos los datos y las etiquetas usando la función ppreviamente creada"]},{"cell_type":"code","execution_count":8,"metadata":{},"outputs":[],"source":["from keras.utils import to_categorical\n","\n","train_directory = \"D:/0.ICAI/Procesamiento_de_dato_no_estructurado/SentimentAnalysis/imagen/Grayscale_Face_images/train\"\n","test_directory = \"D:/0.ICAI/Procesamiento_de_dato_no_estructurado/SentimentAnalysis/imagen/Grayscale_Face_images/test\"\n","\n","train_images, train_labels = load_images_from_subfolders(train_directory)\n","test_images, test_labels = load_images_from_subfolders(test_directory)\n","\n","\n","train_images = train_images.reshape((5627, 224, 224, 1))\n","train_images = train_images.astype('float32') / 255\n","\n","test_images = test_images.reshape((1220, 224, 224, 1))\n","test_images = test_images.astype('float32') / 255\n","\n","train_labels = to_categorical(train_labels)\n","test_labels = to_categorical(test_labels)\n","\n"]},{"cell_type":"markdown","metadata":{},"source":["# Entrenamiento\n","\n","Para el entrenamiento utilizamos el optimizador adam y este modelo será entrenado con un bath_size de 64 lo qeu ayudará a que converga el modelo y lo entrenaremos con 10 epochs.\n"]},{"cell_type":"code","execution_count":9,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":608},"executionInfo":{"elapsed":250296,"status":"ok","timestamp":1581201183751,"user":{"displayName":"Ana Laguna Pradas","photoUrl":"https://lh3.googleusercontent.com/a-/AAuE7mBSGptTK-p_QVjyTUyfgnUmNBGxmgf1siaiZ-4m=s64","userId":"04586459829917851529"},"user_tz":-60},"id":"AtEuCYKgxBLy","outputId":"31e66fde-4076-49ce-868e-daebb6c76528"},"outputs":[{"name":"stdout","output_type":"stream","text":["Epoch 1/10\n","88/88 [==============================] - 23s 194ms/step - loss: 2.1015 - accuracy: 0.1480\n","Epoch 2/10\n","88/88 [==============================] - 16s 179ms/step - loss: 1.9498 - accuracy: 0.2349\n","Epoch 3/10\n","88/88 [==============================] - 16s 179ms/step - loss: 1.8389 - accuracy: 0.2934\n","Epoch 4/10\n","88/88 [==============================] - 16s 179ms/step - loss: 1.7463 - accuracy: 0.3311\n","Epoch 5/10\n","88/88 [==============================] - 16s 179ms/step - loss: 1.6438 - accuracy: 0.3734\n","Epoch 6/10\n","88/88 [==============================] - 16s 180ms/step - loss: 1.5760 - accuracy: 0.3919\n","Epoch 7/10\n","88/88 [==============================] - 16s 179ms/step - loss: 1.5130 - accuracy: 0.4246\n","Epoch 8/10\n","88/88 [==============================] - 16s 179ms/step - loss: 1.4560 - accuracy: 0.4521\n","Epoch 9/10\n","88/88 [==============================] - 16s 179ms/step - loss: 1.3668 - accuracy: 0.4811\n","Epoch 10/10\n","88/88 [==============================] - 16s 179ms/step - loss: 1.2761 - accuracy: 0.5097\n"]},{"data":{"text/plain":["<keras.callbacks.History at 0x1fa35c8a730>"]},"execution_count":9,"metadata":{},"output_type":"execute_result"}],"source":["model.compile(optimizer='adam',\n","              loss='categorical_crossentropy',\n","              metrics=['accuracy'])\n","model.fit(train_images, train_labels, epochs=10, batch_size=64)"]},{"cell_type":"markdown","metadata":{},"source":["Guardamos el modelo"]},{"cell_type":"code","execution_count":14,"metadata":{},"outputs":[],"source":["# Guardar el modelo entrenado en disco\n","model.save('models/models_from_scratch/m1_adam-10epochs-batch_size64.h5')"]},{"cell_type":"markdown","metadata":{},"source":["Cargamos el modelo"]},{"cell_type":"code","execution_count":11,"metadata":{},"outputs":[{"data":{"text/plain":["<keras.engine.sequential.Sequential at 0x1fa172211f0>"]},"execution_count":11,"metadata":{},"output_type":"execute_result"}],"source":["from keras.models import load_model\n","\n","# Cargar el modelo desde el archivo guardado\n","modelo_cargado = load_model('models/models_from_scratch/m1_adam-10epochs-batch_size64.h5')\n","modelo_cargado"]},{"cell_type":"markdown","metadata":{"id":"RJIRKuHCxBL1"},"source":["Vemos la accuracy"]},{"cell_type":"code","execution_count":15,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":34},"executionInfo":{"elapsed":3474,"status":"ok","timestamp":1581201980008,"user":{"displayName":"Ana Laguna Pradas","photoUrl":"https://lh3.googleusercontent.com/a-/AAuE7mBSGptTK-p_QVjyTUyfgnUmNBGxmgf1siaiZ-4m=s64","userId":"04586459829917851529"},"user_tz":-60},"id":"0iZtcbAuxBL2","outputId":"a1e51c8f-9796-460b-8481-52745d18c35e"},"outputs":[{"name":"stdout","output_type":"stream","text":["39/39 [==============================] - 1s 27ms/step - loss: 1.6394 - accuracy: 0.3967\n"]}],"source":["test_loss, test_acc = model.evaluate(test_images, test_labels)"]},{"cell_type":"code","execution_count":16,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":34},"executionInfo":{"elapsed":605,"status":"ok","timestamp":1581201983105,"user":{"displayName":"Ana Laguna Pradas","photoUrl":"https://lh3.googleusercontent.com/a-/AAuE7mBSGptTK-p_QVjyTUyfgnUmNBGxmgf1siaiZ-4m=s64","userId":"04586459829917851529"},"user_tz":-60},"id":"42h_HrnJxBL5","outputId":"ce3cda37-6a67-453d-f774-f2a8d6e7d919"},"outputs":[{"data":{"text/plain":["0.3967213034629822"]},"execution_count":16,"metadata":{},"output_type":"execute_result"}],"source":["test_acc"]}],"metadata":{"colab":{"collapsed_sections":[],"name":"6_IMAGE_Introduction-to-convnets.ipynb","provenance":[]},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.9.19"}},"nbformat":4,"nbformat_minor":0}
