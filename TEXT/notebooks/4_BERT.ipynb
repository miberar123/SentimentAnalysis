{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# __BERT__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we aimed to perform the text classification using the DistilBERT model, a distilled version of BERT (Bidirectional Encoder Representations from Transformers), implemented in TensorFlow. We utilized the DistilBERT model due to its efficiency and effectiveness in natural language processing tasks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: transformers in c:\\users\\ainhoa\\anaconda3\\envs\\ml\\lib\\site-packages (4.40.2)\n",
      "Requirement already satisfied: datasets in c:\\users\\ainhoa\\anaconda3\\envs\\ml\\lib\\site-packages (2.19.1)\n",
      "Requirement already satisfied: filelock in c:\\users\\ainhoa\\anaconda3\\envs\\ml\\lib\\site-packages (from transformers) (3.13.1)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.19.3 in c:\\users\\ainhoa\\anaconda3\\envs\\ml\\lib\\site-packages (from transformers) (0.23.0)\n",
      "Requirement already satisfied: numpy>=1.17 in c:\\users\\ainhoa\\anaconda3\\envs\\ml\\lib\\site-packages (from transformers) (1.26.4)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\ainhoa\\anaconda3\\envs\\ml\\lib\\site-packages (from transformers) (23.1)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\users\\ainhoa\\anaconda3\\envs\\ml\\lib\\site-packages (from transformers) (6.0)\n",
      "Requirement already satisfied: regex!=2019.12.17 in c:\\users\\ainhoa\\anaconda3\\envs\\ml\\lib\\site-packages (from transformers) (2024.5.10)\n",
      "Requirement already satisfied: requests in c:\\users\\ainhoa\\anaconda3\\envs\\ml\\lib\\site-packages (from transformers) (2.31.0)\n",
      "Requirement already satisfied: tokenizers<0.20,>=0.19 in c:\\users\\ainhoa\\anaconda3\\envs\\ml\\lib\\site-packages (from transformers) (0.19.1)\n",
      "Requirement already satisfied: safetensors>=0.4.1 in c:\\users\\ainhoa\\anaconda3\\envs\\ml\\lib\\site-packages (from transformers) (0.4.3)\n",
      "Requirement already satisfied: tqdm>=4.27 in c:\\users\\ainhoa\\anaconda3\\envs\\ml\\lib\\site-packages (from transformers) (4.66.1)\n",
      "Requirement already satisfied: pyarrow>=12.0.0 in c:\\users\\ainhoa\\anaconda3\\envs\\ml\\lib\\site-packages (from datasets) (15.0.1)\n",
      "Requirement already satisfied: pyarrow-hotfix in c:\\users\\ainhoa\\anaconda3\\envs\\ml\\lib\\site-packages (from datasets) (0.6)\n",
      "Requirement already satisfied: dill<0.3.9,>=0.3.0 in c:\\users\\ainhoa\\anaconda3\\envs\\ml\\lib\\site-packages (from datasets) (0.3.8)\n",
      "Requirement already satisfied: pandas in c:\\users\\ainhoa\\anaconda3\\envs\\ml\\lib\\site-packages (from datasets) (2.0.3)\n",
      "Requirement already satisfied: xxhash in c:\\users\\ainhoa\\anaconda3\\envs\\ml\\lib\\site-packages (from datasets) (3.4.1)\n",
      "Requirement already satisfied: multiprocess in c:\\users\\ainhoa\\anaconda3\\envs\\ml\\lib\\site-packages (from datasets) (0.70.16)\n",
      "Requirement already satisfied: fsspec[http]<=2024.3.1,>=2023.1.0 in c:\\users\\ainhoa\\anaconda3\\envs\\ml\\lib\\site-packages (from datasets) (2023.9.2)\n",
      "Requirement already satisfied: aiohttp in c:\\users\\ainhoa\\anaconda3\\envs\\ml\\lib\\site-packages (from datasets) (3.9.5)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in c:\\users\\ainhoa\\anaconda3\\envs\\ml\\lib\\site-packages (from aiohttp->datasets) (1.3.1)\n",
      "Requirement already satisfied: attrs>=17.3.0 in c:\\users\\ainhoa\\anaconda3\\envs\\ml\\lib\\site-packages (from aiohttp->datasets) (23.1.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in c:\\users\\ainhoa\\anaconda3\\envs\\ml\\lib\\site-packages (from aiohttp->datasets) (1.4.1)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in c:\\users\\ainhoa\\anaconda3\\envs\\ml\\lib\\site-packages (from aiohttp->datasets) (6.0.5)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in c:\\users\\ainhoa\\anaconda3\\envs\\ml\\lib\\site-packages (from aiohttp->datasets) (1.9.4)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in c:\\users\\ainhoa\\anaconda3\\envs\\ml\\lib\\site-packages (from huggingface-hub<1.0,>=0.19.3->transformers) (4.8.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\ainhoa\\anaconda3\\envs\\ml\\lib\\site-packages (from requests->transformers) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\ainhoa\\anaconda3\\envs\\ml\\lib\\site-packages (from requests->transformers) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\ainhoa\\anaconda3\\envs\\ml\\lib\\site-packages (from requests->transformers) (1.26.18)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\ainhoa\\anaconda3\\envs\\ml\\lib\\site-packages (from requests->transformers) (2023.7.22)\n",
      "Requirement already satisfied: colorama in c:\\users\\ainhoa\\anaconda3\\envs\\ml\\lib\\site-packages (from tqdm>=4.27->transformers) (0.4.6)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\ainhoa\\anaconda3\\envs\\ml\\lib\\site-packages (from pandas->datasets) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\ainhoa\\anaconda3\\envs\\ml\\lib\\site-packages (from pandas->datasets) (2023.3.post1)\n",
      "Requirement already satisfied: tzdata>=2022.1 in c:\\users\\ainhoa\\anaconda3\\envs\\ml\\lib\\site-packages (from pandas->datasets) (2023.3)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\ainhoa\\anaconda3\\envs\\ml\\lib\\site-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.16.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install transformers datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: tf-keras in c:\\users\\ainhoa\\anaconda3\\envs\\ml\\lib\\site-packages (2.16.0)\n",
      "Requirement already satisfied: tensorflow<2.17,>=2.16 in c:\\users\\ainhoa\\anaconda3\\envs\\ml\\lib\\site-packages (from tf-keras) (2.16.1)\n",
      "Requirement already satisfied: tensorflow-intel==2.16.1 in c:\\users\\ainhoa\\anaconda3\\envs\\ml\\lib\\site-packages (from tensorflow<2.17,>=2.16->tf-keras) (2.16.1)\n",
      "Requirement already satisfied: absl-py>=1.0.0 in c:\\users\\ainhoa\\anaconda3\\envs\\ml\\lib\\site-packages (from tensorflow-intel==2.16.1->tensorflow<2.17,>=2.16->tf-keras) (2.1.0)\n",
      "Requirement already satisfied: astunparse>=1.6.0 in c:\\users\\ainhoa\\anaconda3\\envs\\ml\\lib\\site-packages (from tensorflow-intel==2.16.1->tensorflow<2.17,>=2.16->tf-keras) (1.6.3)\n",
      "Requirement already satisfied: flatbuffers>=23.5.26 in c:\\users\\ainhoa\\anaconda3\\envs\\ml\\lib\\site-packages (from tensorflow-intel==2.16.1->tensorflow<2.17,>=2.16->tf-keras) (24.3.25)\n",
      "Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in c:\\users\\ainhoa\\anaconda3\\envs\\ml\\lib\\site-packages (from tensorflow-intel==2.16.1->tensorflow<2.17,>=2.16->tf-keras) (0.5.4)\n",
      "Requirement already satisfied: google-pasta>=0.1.1 in c:\\users\\ainhoa\\anaconda3\\envs\\ml\\lib\\site-packages (from tensorflow-intel==2.16.1->tensorflow<2.17,>=2.16->tf-keras) (0.2.0)\n",
      "Requirement already satisfied: h5py>=3.10.0 in c:\\users\\ainhoa\\anaconda3\\envs\\ml\\lib\\site-packages (from tensorflow-intel==2.16.1->tensorflow<2.17,>=2.16->tf-keras) (3.11.0)\n",
      "Requirement already satisfied: libclang>=13.0.0 in c:\\users\\ainhoa\\anaconda3\\envs\\ml\\lib\\site-packages (from tensorflow-intel==2.16.1->tensorflow<2.17,>=2.16->tf-keras) (18.1.1)\n",
      "Requirement already satisfied: ml-dtypes~=0.3.1 in c:\\users\\ainhoa\\anaconda3\\envs\\ml\\lib\\site-packages (from tensorflow-intel==2.16.1->tensorflow<2.17,>=2.16->tf-keras) (0.3.2)\n",
      "Requirement already satisfied: opt-einsum>=2.3.2 in c:\\users\\ainhoa\\anaconda3\\envs\\ml\\lib\\site-packages (from tensorflow-intel==2.16.1->tensorflow<2.17,>=2.16->tf-keras) (3.3.0)\n",
      "Requirement already satisfied: packaging in c:\\users\\ainhoa\\anaconda3\\envs\\ml\\lib\\site-packages (from tensorflow-intel==2.16.1->tensorflow<2.17,>=2.16->tf-keras) (23.1)\n",
      "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.20.3 in c:\\users\\ainhoa\\anaconda3\\envs\\ml\\lib\\site-packages (from tensorflow-intel==2.16.1->tensorflow<2.17,>=2.16->tf-keras) (4.25.3)\n",
      "Requirement already satisfied: requests<3,>=2.21.0 in c:\\users\\ainhoa\\anaconda3\\envs\\ml\\lib\\site-packages (from tensorflow-intel==2.16.1->tensorflow<2.17,>=2.16->tf-keras) (2.31.0)\n",
      "Requirement already satisfied: setuptools in c:\\users\\ainhoa\\anaconda3\\envs\\ml\\lib\\site-packages (from tensorflow-intel==2.16.1->tensorflow<2.17,>=2.16->tf-keras) (68.0.0)\n",
      "Requirement already satisfied: six>=1.12.0 in c:\\users\\ainhoa\\anaconda3\\envs\\ml\\lib\\site-packages (from tensorflow-intel==2.16.1->tensorflow<2.17,>=2.16->tf-keras) (1.16.0)\n",
      "Requirement already satisfied: termcolor>=1.1.0 in c:\\users\\ainhoa\\anaconda3\\envs\\ml\\lib\\site-packages (from tensorflow-intel==2.16.1->tensorflow<2.17,>=2.16->tf-keras) (2.4.0)\n",
      "Requirement already satisfied: typing-extensions>=3.6.6 in c:\\users\\ainhoa\\anaconda3\\envs\\ml\\lib\\site-packages (from tensorflow-intel==2.16.1->tensorflow<2.17,>=2.16->tf-keras) (4.8.0)\n",
      "Requirement already satisfied: wrapt>=1.11.0 in c:\\users\\ainhoa\\anaconda3\\envs\\ml\\lib\\site-packages (from tensorflow-intel==2.16.1->tensorflow<2.17,>=2.16->tf-keras) (1.14.1)\n",
      "Requirement already satisfied: grpcio<2.0,>=1.24.3 in c:\\users\\ainhoa\\anaconda3\\envs\\ml\\lib\\site-packages (from tensorflow-intel==2.16.1->tensorflow<2.17,>=2.16->tf-keras) (1.63.0)\n",
      "Requirement already satisfied: tensorboard<2.17,>=2.16 in c:\\users\\ainhoa\\anaconda3\\envs\\ml\\lib\\site-packages (from tensorflow-intel==2.16.1->tensorflow<2.17,>=2.16->tf-keras) (2.16.2)\n",
      "Requirement already satisfied: keras>=3.0.0 in c:\\users\\ainhoa\\anaconda3\\envs\\ml\\lib\\site-packages (from tensorflow-intel==2.16.1->tensorflow<2.17,>=2.16->tf-keras) (3.3.3)\n",
      "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in c:\\users\\ainhoa\\anaconda3\\envs\\ml\\lib\\site-packages (from tensorflow-intel==2.16.1->tensorflow<2.17,>=2.16->tf-keras) (0.31.0)\n",
      "Requirement already satisfied: numpy<2.0.0,>=1.23.5 in c:\\users\\ainhoa\\anaconda3\\envs\\ml\\lib\\site-packages (from tensorflow-intel==2.16.1->tensorflow<2.17,>=2.16->tf-keras) (1.26.4)\n",
      "Requirement already satisfied: wheel<1.0,>=0.23.0 in c:\\users\\ainhoa\\anaconda3\\envs\\ml\\lib\\site-packages (from astunparse>=1.6.0->tensorflow-intel==2.16.1->tensorflow<2.17,>=2.16->tf-keras) (0.41.2)\n",
      "Requirement already satisfied: rich in c:\\users\\ainhoa\\anaconda3\\envs\\ml\\lib\\site-packages (from keras>=3.0.0->tensorflow-intel==2.16.1->tensorflow<2.17,>=2.16->tf-keras) (13.7.1)\n",
      "Requirement already satisfied: namex in c:\\users\\ainhoa\\anaconda3\\envs\\ml\\lib\\site-packages (from keras>=3.0.0->tensorflow-intel==2.16.1->tensorflow<2.17,>=2.16->tf-keras) (0.0.8)\n",
      "Requirement already satisfied: optree in c:\\users\\ainhoa\\anaconda3\\envs\\ml\\lib\\site-packages (from keras>=3.0.0->tensorflow-intel==2.16.1->tensorflow<2.17,>=2.16->tf-keras) (0.11.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\ainhoa\\anaconda3\\envs\\ml\\lib\\site-packages (from requests<3,>=2.21.0->tensorflow-intel==2.16.1->tensorflow<2.17,>=2.16->tf-keras) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\ainhoa\\anaconda3\\envs\\ml\\lib\\site-packages (from requests<3,>=2.21.0->tensorflow-intel==2.16.1->tensorflow<2.17,>=2.16->tf-keras) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\ainhoa\\anaconda3\\envs\\ml\\lib\\site-packages (from requests<3,>=2.21.0->tensorflow-intel==2.16.1->tensorflow<2.17,>=2.16->tf-keras) (1.26.18)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\ainhoa\\anaconda3\\envs\\ml\\lib\\site-packages (from requests<3,>=2.21.0->tensorflow-intel==2.16.1->tensorflow<2.17,>=2.16->tf-keras) (2023.7.22)\n",
      "Requirement already satisfied: markdown>=2.6.8 in c:\\users\\ainhoa\\anaconda3\\envs\\ml\\lib\\site-packages (from tensorboard<2.17,>=2.16->tensorflow-intel==2.16.1->tensorflow<2.17,>=2.16->tf-keras) (3.6)\n",
      "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in c:\\users\\ainhoa\\anaconda3\\envs\\ml\\lib\\site-packages (from tensorboard<2.17,>=2.16->tensorflow-intel==2.16.1->tensorflow<2.17,>=2.16->tf-keras) (0.7.2)\n",
      "Requirement already satisfied: werkzeug>=1.0.1 in c:\\users\\ainhoa\\anaconda3\\envs\\ml\\lib\\site-packages (from tensorboard<2.17,>=2.16->tensorflow-intel==2.16.1->tensorflow<2.17,>=2.16->tf-keras) (3.0.2)\n",
      "Requirement already satisfied: MarkupSafe>=2.1.1 in c:\\users\\ainhoa\\anaconda3\\envs\\ml\\lib\\site-packages (from werkzeug>=1.0.1->tensorboard<2.17,>=2.16->tensorflow-intel==2.16.1->tensorflow<2.17,>=2.16->tf-keras) (2.1.1)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in c:\\users\\ainhoa\\anaconda3\\envs\\ml\\lib\\site-packages (from rich->keras>=3.0.0->tensorflow-intel==2.16.1->tensorflow<2.17,>=2.16->tf-keras) (3.0.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in c:\\users\\ainhoa\\anaconda3\\envs\\ml\\lib\\site-packages (from rich->keras>=3.0.0->tensorflow-intel==2.16.1->tensorflow<2.17,>=2.16->tf-keras) (2.15.1)\n",
      "Requirement already satisfied: mdurl~=0.1 in c:\\users\\ainhoa\\anaconda3\\envs\\ml\\lib\\site-packages (from markdown-it-py>=2.2.0->rich->keras>=3.0.0->tensorflow-intel==2.16.1->tensorflow<2.17,>=2.16->tf-keras) (0.1.2)\n"
     ]
    }
   ],
   "source": [
    "!pip install tf-keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import csv\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import classification_report\n",
    "#from transformers import AutoModelForSequenceClassification, TFDistilBertForSequenceClassification, TFTrainingArguments, TFTrainer\n",
    "\n",
    "from tensorflow.keras.layers import Embedding, Conv1D, MaxPooling1D, Flatten, Dense, Dropout\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "\n",
    "# Custom libraries\n",
    "import sys\n",
    "sys.path.append('..')\n",
    "from functions.models import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We started by loading the training, testing, and validation datasets from TSV files using Pandas. Each dataset consisted of text comments and their corresponding labels. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_path = '../data/X_train.tsv'\n",
    "test_path = '../data/X_test.tsv'\n",
    "validation_path = '../data/X_val.tsv'\n",
    "\n",
    "X_train = pd.read_csv(train_path, sep='\\t')\n",
    "X_test = pd.read_csv(test_path, sep='\\t')\n",
    "X_val = pd.read_csv(validation_path, sep='\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_path = '../data/y_train.tsv'\n",
    "test_path = '../data/y_test.tsv'\n",
    "validation_path = '../data/y_val.tsv'\n",
    "\n",
    "y_train = pd.read_csv(train_path, sep='\\t')\n",
    "y_test = pd.read_csv(test_path, sep='\\t')\n",
    "y_val = pd.read_csv(validation_path, sep='\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datasets\n",
    "\n",
    "train_dataset = datasets.Dataset.from_dict({\"text\": X_train[\"comment\"], \"label\": y_train[\"label\"]})\n",
    "test_dataset = datasets.Dataset.from_dict({\"text\": X_test[\"comment\"], \"label\": y_test[\"label\"]})\n",
    "validation_dataset = datasets.Dataset.from_dict({\"text\": X_val[\"comment\"], \"label\": y_val[\"label\"]})\n",
    "\n",
    "dataset = datasets.DatasetDict({\"train\": train_dataset, \"test\": test_dataset, \"validation\": validation_dataset})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'train': (39900, 2), 'test': (4833, 2), 'validation': (4891, 2)}"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenization, Padding and Sequencing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We then tokenized the text data using the DistilBERT tokenizer, which converts text inputs into numerical vectors that the model can process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Ainhoa\\anaconda3\\envs\\ML\\Lib\\site-packages\\huggingface_hub\\file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"distilbert-base-uncased\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_function(examples):\n",
    "    return tokenizer(examples[\"text\"], truncation=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a8817d7a4cba4c6c801c84b01a9fdd5d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/39900 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "93b48fc77c6944c4b3cc5344a9d47236",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/4833 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b633b9ee0b7c44fcb0741b3fba46e0e5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/4891 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "tokenized_dataset = dataset.map(preprocess_function, batched=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import DataCollatorWithPadding\n",
    "\n",
    "data_collator = DataCollatorWithPadding(tokenizer=tokenizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Building"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For our text classification model, we utilized the AutoModelForSequenceClassification class from the transformers library. This class automatically loads the pre-trained DistilBERT model fine-tuned for sequence classification tasks. We specified the number of labels based on the unique labels present in the training data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Ainhoa\\anaconda3\\envs\\ML\\Lib\\site-packages\\huggingface_hub\\file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForSequenceClassification\n",
    "\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\"distilbert-base-uncased\", num_labels=y_train[\"label\"].nunique())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import TrainingArguments, Trainer\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./results\",\n",
    "    learning_rate=2e-5,\n",
    "    per_device_train_batch_size=16,\n",
    "    per_device_eval_batch_size=16,\n",
    "    num_train_epochs=5,\n",
    "    weight_decay=0.01,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We defined the training configuration using the TrainingArguments class, setting parameters such as the learning rate, batch size, number of epochs, and weight decay. The Trainer class from the transformers library was employed to facilitate model training. We provided the model, training arguments, tokenized training dataset, evaluation dataset, tokenizer, and data collator to the Trainer object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "41b2389edd2143f7b408541578be951d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/12470 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 2.5003, 'grad_norm': 6.8331170082092285, 'learning_rate': 1.9198075380914196e-05, 'epoch': 0.2}\n",
      "{'loss': 2.154, 'grad_norm': 8.398826599121094, 'learning_rate': 1.8396150761828387e-05, 'epoch': 0.4}\n",
      "{'loss': 2.0152, 'grad_norm': 10.167052268981934, 'learning_rate': 1.7594226142742585e-05, 'epoch': 0.6}\n",
      "{'loss': 2.0048, 'grad_norm': 11.380202293395996, 'learning_rate': 1.679230152365678e-05, 'epoch': 0.8}\n",
      "{'loss': 1.9495, 'grad_norm': 7.773941993713379, 'learning_rate': 1.5990376904570973e-05, 'epoch': 1.0}\n",
      "{'loss': 1.8047, 'grad_norm': 7.956082820892334, 'learning_rate': 1.5188452285485164e-05, 'epoch': 1.2}\n",
      "{'loss': 1.8158, 'grad_norm': 11.588322639465332, 'learning_rate': 1.4386527666399359e-05, 'epoch': 1.4}\n",
      "{'loss': 1.815, 'grad_norm': 8.168700218200684, 'learning_rate': 1.3584603047313553e-05, 'epoch': 1.6}\n",
      "{'loss': 1.8091, 'grad_norm': 9.24846363067627, 'learning_rate': 1.2782678428227749e-05, 'epoch': 1.8}\n",
      "{'loss': 1.8097, 'grad_norm': 8.791220664978027, 'learning_rate': 1.198075380914194e-05, 'epoch': 2.0}\n",
      "{'loss': 1.6372, 'grad_norm': 10.190134048461914, 'learning_rate': 1.1178829190056134e-05, 'epoch': 2.21}\n",
      "{'loss': 1.651, 'grad_norm': 11.821816444396973, 'learning_rate': 1.037690457097033e-05, 'epoch': 2.41}\n",
      "{'loss': 1.6477, 'grad_norm': 8.943058967590332, 'learning_rate': 9.574979951884523e-06, 'epoch': 2.61}\n",
      "{'loss': 1.6672, 'grad_norm': 15.206653594970703, 'learning_rate': 8.773055332798717e-06, 'epoch': 2.81}\n",
      "{'loss': 1.6577, 'grad_norm': 12.126842498779297, 'learning_rate': 7.971130713712912e-06, 'epoch': 3.01}\n",
      "{'loss': 1.5164, 'grad_norm': 11.507161140441895, 'learning_rate': 7.169206094627106e-06, 'epoch': 3.21}\n",
      "{'loss': 1.5207, 'grad_norm': 11.055439949035645, 'learning_rate': 6.3672814755413e-06, 'epoch': 3.41}\n",
      "{'loss': 1.5176, 'grad_norm': 9.741928100585938, 'learning_rate': 5.565356856455494e-06, 'epoch': 3.61}\n",
      "{'loss': 1.514, 'grad_norm': 12.360546112060547, 'learning_rate': 4.7634322373696875e-06, 'epoch': 3.81}\n",
      "{'loss': 1.5359, 'grad_norm': 13.247875213623047, 'learning_rate': 3.961507618283882e-06, 'epoch': 4.01}\n",
      "{'loss': 1.3976, 'grad_norm': 14.034493446350098, 'learning_rate': 3.1595829991980757e-06, 'epoch': 4.21}\n",
      "{'loss': 1.4172, 'grad_norm': 16.408279418945312, 'learning_rate': 2.3576583801122697e-06, 'epoch': 4.41}\n",
      "{'loss': 1.4263, 'grad_norm': 13.701294898986816, 'learning_rate': 1.5557337610264636e-06, 'epoch': 4.61}\n",
      "{'loss': 1.4185, 'grad_norm': 15.095510482788086, 'learning_rate': 7.538091419406576e-07, 'epoch': 4.81}\n",
      "{'train_runtime': 15500.0635, 'train_samples_per_second': 12.871, 'train_steps_per_second': 0.805, 'train_loss': 1.706114694798767, 'epoch': 5.0}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=12470, training_loss=1.706114694798767, metrics={'train_runtime': 15500.0635, 'train_samples_per_second': 12.871, 'train_steps_per_second': 0.805, 'total_flos': 785712641419104.0, 'train_loss': 1.706114694798767, 'epoch': 5.0})"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_dataset[\"train\"],\n",
    "    eval_dataset=tokenized_dataset[\"test\"],\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=data_collator,\n",
    ")\n",
    "\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After training the model, we evaluated its performance on the evaluation dataset. We obtained the model's predictions on the evaluation dataset using the predict method of the Trainer object. Then, we computed the accuracy of the model by comparing the predicted classes with the true labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "aa09eb5ead9e41f88e4d1329f782e1ca",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/303 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "{'eval_loss': 1.9426915645599365,\n",
       " 'eval_runtime': 57.9295,\n",
       " 'eval_samples_per_second': 83.429,\n",
       " 'eval_steps_per_second': 5.23,\n",
       " 'epoch': 5.0}"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# See loss\n",
    "trainer.evaluate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "99d8ebca464341b6b235d8323b873032",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/303 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.45934202358783366\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Obtener las predicciones del conjunto de evaluación\n",
    "eval_predictions = trainer.predict(tokenized_dataset[\"test\"]).predictions\n",
    "\n",
    "# Obtener las etiquetas verdaderas del conjunto de evaluación\n",
    "eval_labels = tokenized_dataset[\"test\"][\"label\"]\n",
    "\n",
    "# Calcular las predicciones finales (clase predicha) usando la función argmax\n",
    "predicted_classes = np.argmax(eval_predictions, axis=1)\n",
    "\n",
    "# Calcular la precisión comparando las etiquetas verdaderas con las predicciones\n",
    "accuracy = np.mean(predicted_classes == eval_labels)\n",
    "print(\"Accuracy:\", accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The DistilBERT model achieved an accuracy of 0.46 on the evaluation dataset. By leveraging transformer-based models, such as DistilBERT, we achieved enhanced performance in categorizing text comments into predefined labels. This highlights the efficacy of transformer architectures in capturing complex textual patterns and underscores their potential for various natural language processing applications."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ADNEnv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
