{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# __DIGIBERT__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pre-trained models have revolutionized the field of natural language processing (NLP) by offering a solution to the longstanding challenge of effectively leveraging large-scale unlabeled text data. These models are neural network architectures that have been trained on vast amounts of text data from diverse sources, such as books, articles, and websites. The key idea behind pre-trained models is to learn rich representations of language that capture semantic, syntactic, and contextual information from the text."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "BERT, developed by researchers at Google, is one of the most influential pre-trained models in NLP. It introduced the concept of bidirectional training for transformers, allowing the model to capture context from both left and right directions in a sequence. BERT's architecture consists of transformer layers, which use self-attention mechanisms to weigh the importance of different words in a sentence based on their contextual relevance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the realm of text-based applications, DiGiBERT presents several advantages. It can undergo fine-tuning for tasks such as sentiment analysis, document classification, text summarization, and question answering. Moreover, DiGiBERT's integration of linguistic and contextual information enhances its ability to comprehend and analyze textual data across different domains with superior accuracy and relevance compared to conventional pre-trained models like BERT. This capability opens up avenues for more effective natural language understanding and processing in fields ranging from social media analytics to customer support automation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import csv\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import classification_report\n",
    "#from transformers import AutoModelForSequenceClassification, TFDistilBertForSequenceClassification, TFTrainingArguments, TFTrainer\n",
    "\n",
    "from tensorflow.keras.layers import Embedding, Conv1D, MaxPooling1D, Flatten, Dense, Dropout\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "\n",
    "# Custom libraries\n",
    "import sys\n",
    "sys.path.append('..')\n",
    "from functions.models import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We started by loading the training, testing, and validation datasets from TSV files using Pandas. Each dataset consisted of text comments and their corresponding labels. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_path = '../data/X_train.tsv'\n",
    "test_path = '../data/X_test.tsv'\n",
    "validation_path = '../data/X_val.tsv'\n",
    "\n",
    "X_train = pd.read_csv(train_path, sep='\\t')\n",
    "X_test = pd.read_csv(test_path, sep='\\t')\n",
    "X_val = pd.read_csv(validation_path, sep='\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_path = '../data/y_train.tsv'\n",
    "test_path = '../data/y_test.tsv'\n",
    "validation_path = '../data/y_val.tsv'\n",
    "\n",
    "y_train = pd.read_csv(train_path, sep='\\t')\n",
    "y_test = pd.read_csv(test_path, sep='\\t')\n",
    "y_val = pd.read_csv(validation_path, sep='\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datasets\n",
    "\n",
    "train_dataset = datasets.Dataset.from_dict({\"text\": X_train[\"comment\"], \"label\": y_train[\"label\"]})\n",
    "test_dataset = datasets.Dataset.from_dict({\"text\": X_test[\"comment\"], \"label\": y_test[\"label\"]})\n",
    "validation_dataset = datasets.Dataset.from_dict({\"text\": X_val[\"comment\"], \"label\": y_val[\"label\"]})\n",
    "\n",
    "dataset = datasets.DatasetDict({\"train\": train_dataset, \"test\": test_dataset, \"validation\": validation_dataset})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'train': (39900, 2), 'test': (4833, 2), 'validation': (4891, 2)}"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenization, Padding and Sequencing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We then tokenized the text data using the DistilBERT tokenizer, which converts text inputs into numerical vectors that the model can process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Ainhoa\\anaconda3\\envs\\ML\\Lib\\site-packages\\huggingface_hub\\file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"distilbert-base-uncased\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_function(examples):\n",
    "    return tokenizer(examples[\"text\"], truncation=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a8817d7a4cba4c6c801c84b01a9fdd5d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/39900 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "93b48fc77c6944c4b3cc5344a9d47236",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/4833 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b633b9ee0b7c44fcb0741b3fba46e0e5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/4891 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "tokenized_dataset = dataset.map(preprocess_function, batched=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import DataCollatorWithPadding\n",
    "\n",
    "data_collator = DataCollatorWithPadding(tokenizer=tokenizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Building"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For our text classification model, we utilized the AutoModelForSequenceClassification class from the transformers library. This class automatically loads the pre-trained DistilBERT model fine-tuned for sequence classification tasks. We specified the number of labels based on the unique labels present in the training data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Ainhoa\\anaconda3\\envs\\ML\\Lib\\site-packages\\huggingface_hub\\file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForSequenceClassification\n",
    "\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\"distilbert-base-uncased\", num_labels=y_train[\"label\"].nunique())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import TrainingArguments, Trainer\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./results\",\n",
    "    learning_rate=2e-5,\n",
    "    per_device_train_batch_size=16,\n",
    "    per_device_eval_batch_size=16,\n",
    "    num_train_epochs=5,\n",
    "    weight_decay=0.01,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We defined the training configuration using the TrainingArguments class, setting parameters such as the learning rate, batch size, number of epochs, and weight decay. The Trainer class from the transformers library was employed to facilitate model training. We provided the model, training arguments, tokenized training dataset, evaluation dataset, tokenizer, and data collator to the Trainer object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "41b2389edd2143f7b408541578be951d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/12470 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 2.5003, 'grad_norm': 6.8331170082092285, 'learning_rate': 1.9198075380914196e-05, 'epoch': 0.2}\n",
      "{'loss': 2.154, 'grad_norm': 8.398826599121094, 'learning_rate': 1.8396150761828387e-05, 'epoch': 0.4}\n",
      "{'loss': 2.0152, 'grad_norm': 10.167052268981934, 'learning_rate': 1.7594226142742585e-05, 'epoch': 0.6}\n",
      "{'loss': 2.0048, 'grad_norm': 11.380202293395996, 'learning_rate': 1.679230152365678e-05, 'epoch': 0.8}\n",
      "{'loss': 1.9495, 'grad_norm': 7.773941993713379, 'learning_rate': 1.5990376904570973e-05, 'epoch': 1.0}\n",
      "{'loss': 1.8047, 'grad_norm': 7.956082820892334, 'learning_rate': 1.5188452285485164e-05, 'epoch': 1.2}\n",
      "{'loss': 1.8158, 'grad_norm': 11.588322639465332, 'learning_rate': 1.4386527666399359e-05, 'epoch': 1.4}\n",
      "{'loss': 1.815, 'grad_norm': 8.168700218200684, 'learning_rate': 1.3584603047313553e-05, 'epoch': 1.6}\n",
      "{'loss': 1.8091, 'grad_norm': 9.24846363067627, 'learning_rate': 1.2782678428227749e-05, 'epoch': 1.8}\n",
      "{'loss': 1.8097, 'grad_norm': 8.791220664978027, 'learning_rate': 1.198075380914194e-05, 'epoch': 2.0}\n",
      "{'loss': 1.6372, 'grad_norm': 10.190134048461914, 'learning_rate': 1.1178829190056134e-05, 'epoch': 2.21}\n",
      "{'loss': 1.651, 'grad_norm': 11.821816444396973, 'learning_rate': 1.037690457097033e-05, 'epoch': 2.41}\n",
      "{'loss': 1.6477, 'grad_norm': 8.943058967590332, 'learning_rate': 9.574979951884523e-06, 'epoch': 2.61}\n",
      "{'loss': 1.6672, 'grad_norm': 15.206653594970703, 'learning_rate': 8.773055332798717e-06, 'epoch': 2.81}\n",
      "{'loss': 1.6577, 'grad_norm': 12.126842498779297, 'learning_rate': 7.971130713712912e-06, 'epoch': 3.01}\n",
      "{'loss': 1.5164, 'grad_norm': 11.507161140441895, 'learning_rate': 7.169206094627106e-06, 'epoch': 3.21}\n",
      "{'loss': 1.5207, 'grad_norm': 11.055439949035645, 'learning_rate': 6.3672814755413e-06, 'epoch': 3.41}\n",
      "{'loss': 1.5176, 'grad_norm': 9.741928100585938, 'learning_rate': 5.565356856455494e-06, 'epoch': 3.61}\n",
      "{'loss': 1.514, 'grad_norm': 12.360546112060547, 'learning_rate': 4.7634322373696875e-06, 'epoch': 3.81}\n",
      "{'loss': 1.5359, 'grad_norm': 13.247875213623047, 'learning_rate': 3.961507618283882e-06, 'epoch': 4.01}\n",
      "{'loss': 1.3976, 'grad_norm': 14.034493446350098, 'learning_rate': 3.1595829991980757e-06, 'epoch': 4.21}\n",
      "{'loss': 1.4172, 'grad_norm': 16.408279418945312, 'learning_rate': 2.3576583801122697e-06, 'epoch': 4.41}\n",
      "{'loss': 1.4263, 'grad_norm': 13.701294898986816, 'learning_rate': 1.5557337610264636e-06, 'epoch': 4.61}\n",
      "{'loss': 1.4185, 'grad_norm': 15.095510482788086, 'learning_rate': 7.538091419406576e-07, 'epoch': 4.81}\n",
      "{'train_runtime': 15500.0635, 'train_samples_per_second': 12.871, 'train_steps_per_second': 0.805, 'train_loss': 1.706114694798767, 'epoch': 5.0}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=12470, training_loss=1.706114694798767, metrics={'train_runtime': 15500.0635, 'train_samples_per_second': 12.871, 'train_steps_per_second': 0.805, 'total_flos': 785712641419104.0, 'train_loss': 1.706114694798767, 'epoch': 5.0})"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_dataset[\"train\"],\n",
    "    eval_dataset=tokenized_dataset[\"test\"],\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=data_collator,\n",
    ")\n",
    "\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After training the model, we evaluated its performance on the evaluation dataset. We obtained the model's predictions on the evaluation dataset using the predict method of the Trainer object. Then, we computed the accuracy of the model by comparing the predicted classes with the true labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "aa09eb5ead9e41f88e4d1329f782e1ca",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/303 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "{'eval_loss': 1.9426915645599365,\n",
       " 'eval_runtime': 57.9295,\n",
       " 'eval_samples_per_second': 83.429,\n",
       " 'eval_steps_per_second': 5.23,\n",
       " 'epoch': 5.0}"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# See loss\n",
    "trainer.evaluate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "99d8ebca464341b6b235d8323b873032",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/303 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.45934202358783366\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Obtener las predicciones del conjunto de evaluaci贸n\n",
    "eval_predictions = trainer.predict(tokenized_dataset[\"test\"]).predictions\n",
    "\n",
    "# Obtener las etiquetas verdaderas del conjunto de evaluaci贸n\n",
    "eval_labels = tokenized_dataset[\"test\"][\"label\"]\n",
    "\n",
    "# Calcular las predicciones finales (clase predicha) usando la funci贸n argmax\n",
    "predicted_classes = np.argmax(eval_predictions, axis=1)\n",
    "\n",
    "# Calcular la precisi贸n comparando las etiquetas verdaderas con las predicciones\n",
    "accuracy = np.mean(predicted_classes == eval_labels)\n",
    "print(\"Accuracy:\", accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The DistilBERT model achieved an accuracy of 0.46 on the evaluation dataset. By leveraging transformer-based models, such as DistilBERT, we achieved enhanced performance in categorizing text comments into predefined labels. This highlights the efficacy of transformer architectures in capturing complex textual patterns and underscores their potential for various natural language processing applications."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ADNEnv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
